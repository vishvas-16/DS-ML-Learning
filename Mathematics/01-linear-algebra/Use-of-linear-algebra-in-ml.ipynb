{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112e04b8",
   "metadata": {},
   "source": [
    "# Mathantics Notes — Interactive Notebook\n",
    "\n",
    "**Goal:** Turn your handwritten notes into runnable demos showing:\n",
    "- training data (data points) vs hypothesis vs final regression line,\n",
    "- the vector form `h(x) = θᵀ x`,\n",
    "- a simple decision boundary example for classification.\n",
    "\n",
    "This notebook **does not** use gradient descent (per your note). Instead it computes the best-fit line using the **normal equation** (closed-form least squares) so it's easy to understand and run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1660e2d",
   "metadata": {},
   "source": [
    "## Quick conceptual recap (from your photos)\n",
    "1. **Data points (dots)** — these are the training examples (features on x-axis, label on y-axis). They have noise and don't lie exactly on a line.\n",
    "2. **Hypothesis function (dashed line)** — an initial guess for the mapping (for example with random θ values).\n",
    "3. **Regression line (solid line)** — the final fitted model after training (here computed with the normal equation). It won't pass through every point but captures the trend.\n",
    "4. **Vector form** — the model can be written as `h(x) = θᵀ x` where `θ = [b, w]` (bias and weight) and `x = [1, feature]`.\n",
    "5. **Decision boundary** — for classification, a linear function can separate classes; the boundary is where `θᵀ x = 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helper plotting function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (8,5)\n",
    "np.random.seed(0)\n",
    "\n",
    "def plot_data_and_lines(X_raw, y_raw, theta_init=None, theta_final=None, title=''):\n",
    "    plt.scatter(X_raw, y_raw, alpha=0.6, label='training data')\n",
    "    xs = np.array([X_raw.min() - 1, X_raw.max() + 1])\n",
    "    if theta_init is not None:\n",
    "        y_init = theta_init[0] + theta_init[1] * xs\n",
    "        plt.plot(xs, y_init, linestyle='--', label='initial hypothesis (dashed)')\n",
    "    if theta_final is not None:\n",
    "        y_fit = theta_final[0] + theta_final[1] * xs\n",
    "        plt.plot(xs, y_fit, color='red', linewidth=2, label='fitted regression (solid)')\n",
    "    plt.xlabel('feature (x)')\n",
    "    plt.ylabel('label (y)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset similar to your notes (age -> income style)\n",
    "X = np.linspace(20, 45, 60)  # feature (e.g., age)\n",
    "y = 1200 * X + 5000 + np.random.normal(0, 7000, X.shape)  # noisy \"income\" label\n",
    "\n",
    "# Quick scatter\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.title('Training data (noisy)')\n",
    "plt.xlabel('X (age)')\n",
    "plt.ylabel('y (income)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c19df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initial hypothesis (random guess for dashed line)\n",
    "theta_init = np.array([20000., 200.])  # [bias, weight]\n",
    "plot_data_and_lines(X, y, theta_init=theta_init, title='Initial hypothesis (dashed) vs data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aca898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute best-fit line using the normal equation (closed-form least squares)\n",
    "# X matrix with bias column\n",
    "X_mat = np.vstack([np.ones_like(X), X]).T  # shape (n,2)\n",
    "y_vec = y.reshape(-1,1)\n",
    "\n",
    "# normal equation: theta = (X^T X)^{-1} X^T y\n",
    "theta_best = np.linalg.inv(X_mat.T.dot(X_mat)).dot(X_mat.T).dot(y_vec)\n",
    "theta_best = theta_best.flatten()\n",
    "print(f'Learned theta (bias, weight) = {theta_best}')\n",
    "\n",
    "# Plot fitted line\n",
    "plot_data_and_lines(X, y, theta_init=theta_init, theta_final=theta_best, title='Initial hypothesis vs fitted regression (normal equation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vector form h(x) = θᵀ x\n",
    "theta = theta_best.reshape(-1,1)  # (2,1)\n",
    "x_example = np.array([1, 30]).reshape(-1,1)  # bias=1, feature=30\n",
    "y_pred = float(theta.T.dot(x_example))\n",
    "print('theta (bias, weight) =', theta.flatten())\n",
    "print('example feature vector x =', x_example.flatten())\n",
    "print('predicted y = θᵀ x =', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-class toy and a perceptron-style training (manual) to show a decision boundary\n",
    "# generate two clusters (class 0 and class 1)\n",
    "n = 200\n",
    "Xc1 = np.random.randn(n//2, 2) * 1.2 + np.array([-2, 0])\n",
    "Xc2 = np.random.randn(n//2, 2) * 1.2 + np.array([2, 1])\n",
    "Xc = np.vstack([Xc1, Xc2])\n",
    "yc = np.hstack([np.zeros(n//2), np.ones(n//2)])\n",
    "\n",
    "# Add bias column for perceptron\n",
    "Xc_bias = np.hstack([np.ones((Xc.shape[0],1)), Xc])  # shape (n,3)\n",
    "\n",
    "# Perceptron training\n",
    "w = np.zeros(3)\n",
    "lr = 0.1\n",
    "for epoch in range(50):\n",
    "    for xi, yi in zip(Xc_bias, yc):\n",
    "        pred = 1 if w.dot(xi) >= 0 else 0\n",
    "        w += lr * (yi - pred) * xi\n",
    "\n",
    "print('learned perceptron weights (bias, w1, w2) =', np.round(w, 3))\n",
    "\n",
    "# plot points and decision boundary\n",
    "plt.scatter(Xc[:,0], Xc[:,1], c=yc, cmap='coolwarm', alpha=0.7)\n",
    "xmin, xmax = Xc[:,0].min()-1, Xc[:,0].max()+1\n",
    "xs = np.linspace(xmin, xmax, 200)\n",
    "# decision boundary w0 + w1*x + w2*y = 0 => y = -(w0 + w1*x)/w2\n",
    "ys = -(w[0] + w[1]*xs) / (w[2] + 1e-9)\n",
    "plt.plot(xs, ys, color='k', linewidth=2)\n",
    "plt.title('Toy 2D classes + decision boundary (perceptron)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db58db",
   "metadata": {},
   "source": [
    "## Takeaways / Notes\n",
    "- The **initial hypothesis** is just a guess (we used a simple numeric example). The **fitted regression** computed with the normal equation finds the best bias & weight minimizing squared error.\n",
    "- The vector form `h(x) = θᵀ x` is a compact way to compute predictions (bias included as `1` in the feature vector).\n",
    "- The **decision boundary** is where `θᵀ x = 0` for classification; we showed a simple perceptron example for a 2D toy dataset.\n",
    "- No gradient descent used — the notebook focuses on intuition and quick closed-form solutions suitable for your current notes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
