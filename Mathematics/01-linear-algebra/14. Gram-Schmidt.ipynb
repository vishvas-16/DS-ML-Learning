{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gram-Schmidt Process\n",
    "\n",
    "### From Linearly Independent Vectors to an Orthonormal Basis\n",
    "\n",
    "This notebook explains the Gram-Schmidt process, a fundamental algorithm in linear algebra for converting a set of linearly independent vectors into an **orthonormal** set that spans the same subspace. The explanation, provides both the theoretical steps and a practical Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need Orthonormal Bases? ðŸ¤”\n",
    "\n",
    "As the notes point out, life in linear algebra is much easier if we can work with an **orthonormal basis**.\n",
    "\n",
    "A set of vectors is **orthonormal** if every vector in the set is:\n",
    "1.  **Orthogonal** to every other vector in the set (their dot product is 0).\n",
    "2.  A **Normal** (or unit) vector, meaning its length (magnitude) is 1.\n",
    "\n",
    "Starting with a set of linearly independent vectors, $V = \\{v_1, v_2, ..., v_m\\}$, which are not necessarily orthogonal or of unit length, the Gram-Schmidt process provides a systematic way to construct a corresponding orthonormal basis $E = \\{e_1, e_2, ..., e_m\\}$.\n",
    "\n",
    "**Key benefits of orthonormal bases:**\n",
    "* **Simpler Computations:** Calculating coordinates and projections becomes much simpler. The coordinates of a vector in an orthonormal basis are just the dot products of the vector with the basis vectors.\n",
    "* **Numerical Stability:** They are preferred in numerical algorithms as they can reduce rounding errors.\n",
    "* **Geometric Intuition:** They align with our standard Cartesian coordinate system ($ \\hat{i}, \\hat{j}, \\hat{k} $), making geometric interpretations more intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Refresher: Normalizing Vectors\n",
    "\n",
    "Before diving into the process, let's clarify what it means to **normalize** a vector.\n",
    "\n",
    "Normalization is the process of scaling a vector so that its length (magnitude or L2-norm) becomes 1, while its direction remains unchanged.\n",
    "\n",
    "For a non-zero vector $w$, its normalized version, $e$, is calculated as:\n",
    "\n",
    "$$ e = \\frac{w}{||w||} $$\n",
    "\n",
    "Where $||w||$ is the magnitude (norm) of $w$.\n",
    "\n",
    "As noted, normalization is crucial for projections. If we project a vector $v$ onto another vector $w$ that is *not* normalized, the length of the projection will be distorted. By normalizing the vector we are projecting onto, we ensure the geometric properties are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector w: [2 3]\n",
      "Magnitude of w: 3.6056\n",
      "Normalized vector e: [0.5547002  0.83205029]\n",
      "Magnitude of e: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a vector\n",
    "w = np.array([2, 3])\n",
    "\n",
    "# Calculate its magnitude (L2 norm)\n",
    "w_norm = np.linalg.norm(w)\n",
    "print(f\"Vector w: {w}\")\n",
    "print(f\"Magnitude of w: {w_norm:.4f}\")\n",
    "\n",
    "# Normalize the vector\n",
    "e = w / w_norm\n",
    "print(f\"Normalized vector e: {e}\")\n",
    "\n",
    "# Verify the magnitude of the normalized vector is 1\n",
    "e_norm = np.linalg.norm(e)\n",
    "print(f\"Magnitude of e: {e_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gram-Schmidt Algorithm: A Step-by-Step Guide\n",
    "\n",
    "Let's start with a set of linearly independent vectors $V = \\{v_1, v_2, v_3, ...\\}$. Our goal is to produce an orthonormal set $E = \\{e_1, e_2, e_3, ...\\}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The First Basis Vector ($e_1$)\n",
    "\n",
    "The first step is the simplest. We take the first vector from our original set, $v_1$, and just normalize it. This becomes our first orthonormal basis vector, $e_1$.\n",
    "\n",
    "$$ e_1 = \\frac{v_1}{||v_1||} $$\n",
    "\n",
    "This vector $e_1$ sets the initial direction for our new basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: The Second Basis Vector ($e_2$)\n",
    "\n",
    "Now, we take the second vector, $v_2$. We need to create a new vector that is orthogonal to our first basis vector, $e_1$. The key idea is to decompose $v_2$ into two components:\n",
    "1.  A component **parallel** to $e_1$.\n",
    "2.  A component **perpendicular** to $e_1$.\n",
    "\n",
    "The parallel component is simply the vector projection of $v_2$ onto $e_1$. Since $e_1$ is already a unit vector, the projection formula is:\n",
    "\n",
    "$$ \\text{proj}_{e_1}(v_2) = (v_2 \\cdot e_1) e_1 $$\n",
    "\n",
    "\n",
    "\n",
    "To get the perpendicular component (which we'll call $w_2$), we subtract the parallel component from the original vector $v_2$:\n",
    "\n",
    "$$ w_2 = v_2 - \\text{proj}_{e_1}(v_2) = v_2 - (v_2 \\cdot e_1) e_1 $$\n",
    "\n",
    "This new vector $w_2$ is, by construction, orthogonal to $e_1$. The final step is to normalize $w_2$ to get our second orthonormal basis vector, $e_2$.\n",
    "\n",
    "$$ e_2 = \\frac{w_2}{||w_2||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Third Basis Vector ($e_3$) and Generalization\n",
    "\n",
    "The process continues in the same way. For the third vector, $v_3$, we want to find a component that is perpendicular to the entire plane spanned by $e_1$ and $e_2$.\n",
    "\n",
    "We do this by subtracting the projections of $v_3$ onto *both* $e_1$ and $e_2$:\n",
    "\n",
    "$$ w_3 = v_3 - \\text{proj}_{e_1}(v_3) - \\text{proj}_{e_2}(v_3) $$\n",
    "$$ w_3 = v_3 - (v_3 \\cdot e_1) e_1 - (v_3 \\cdot e_2) e_2 $$\n",
    "\n",
    "The resulting vector $w_3$ is orthogonal to both $e_1$ and $e_2$. We then normalize it to get $e_3$:\n",
    "\n",
    "$$ e_3 = \\frac{w_3}{||w_3||} $$\n",
    "\n",
    "#### The General Formula\n",
    "\n",
    "For any k-th vector $v_k$, we construct the orthogonal vector $w_k$ by subtracting the projections of $v_k$ onto all the previously computed orthonormal basis vectors ($e_1, e_2, ..., e_{k-1}$):\n",
    "\n",
    "$$ w_k = v_k - \\sum_{i=1}^{k-1} (v_k \\cdot e_i) e_i $$\n",
    "\n",
    "And then we normalize to find $e_k$:\n",
    "\n",
    "$$ e_k = \\frac{w_k}{||w_k||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation Example\n",
    "\n",
    "Let's apply this process to a set of 3 linearly independent vectors in $\\mathbb{R}^3$.\n",
    "\n",
    "We will start with the set $V = \\{v_1, v_2, v_3\\}$ where:\n",
    "* $v_1 = [1, 1, 0]$\n",
    "* $v_2 = [1, 2, 0]$\n",
    "* $v_3 = [2, 1, 2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1 = [0.70710678 0.70710678 0.        ]\n",
      "\n",
      "w2 = [-0.5  0.5  0. ]\n",
      "e2 = [-0.70710678  0.70710678  0.        ]\n",
      "\n",
      "w3 = [ 1.27675648e-15 -3.33066907e-16  2.00000000e+00]\n",
      "e3 = [ 6.38378239e-16 -1.66533454e-16  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Our initial set of linearly independent vectors\n",
    "v1 = np.array([1, 1, 0], dtype=float)\n",
    "v2 = np.array([1, 2, 0], dtype=float)\n",
    "v3 = np.array([2, 1, 2], dtype=float)\n",
    "\n",
    "# --- Step 1: Find e1 ---\n",
    "e1 = v1 / np.linalg.norm(v1)\n",
    "print(f\"e1 = {e1}\\n\")\n",
    "\n",
    "# --- Step 2: Find e2 ---\n",
    "# Project v2 onto e1\n",
    "proj_e1_v2 = np.dot(v2, e1) * e1\n",
    "# Subtract the projection to get the orthogonal vector w2\n",
    "w2 = v2 - proj_e1_v2\n",
    "# Normalize w2 to get e2\n",
    "e2 = w2 / np.linalg.norm(w2)\n",
    "print(f\"w2 = {w2}\")\n",
    "print(f\"e2 = {e2}\\n\")\n",
    "\n",
    "# --- Step 3: Find e3 ---\n",
    "# Project v3 onto e1\n",
    "proj_e1_v3 = np.dot(v3, e1) * e1\n",
    "# Project v3 onto e2\n",
    "proj_e2_v3 = np.dot(v3, e2) * e2\n",
    "# Subtract both projections to get the orthogonal vector w3\n",
    "w3 = v3 - proj_e1_v3 - proj_e2_v3\n",
    "# Normalize w3 to get e3\n",
    "e3 = w3 / np.linalg.norm(w3)\n",
    "print(f\"w3 = {w3}\")\n",
    "print(f\"e3 = {e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Now, let's verify that our new set of vectors $\\{e_1, e_2, e_3\\}$ is indeed orthonormal.\n",
    "\n",
    "1.  **Orthogonality:** The dot product of any two distinct vectors should be 0.\n",
    "2.  **Normality:** The magnitude (norm) of each vector should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our orthonormal basis E:\n",
      "[[ 7.07106781e-01  7.07106781e-01  0.00000000e+00]\n",
      " [-7.07106781e-01  7.07106781e-01  0.00000000e+00]\n",
      " [ 6.38378239e-16 -1.66533454e-16  1.00000000e+00]]\n",
      "\n",
      "--- Orthogonality Check (dot products should be ~0) ---\n",
      "e1 . e2 = 0.0000000000\n",
      "e1 . e3 = 0.0000000000\n",
      "e2 . e3 = -0.0000000000\n",
      "\n",
      "E @ E.T (should be the Identity Matrix):\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1. -0.]\n",
      " [ 0. -0.  1.]]\n",
      "\n",
      "--- Normality Check (magnitudes should be 1) ---\n",
      "||e1|| = 1.0000\n",
      "||e2|| = 1.0000\n",
      "||e3|| = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix from the resulting basis vectors (as rows)\n",
    "E = np.array([e1, e2, e3])\n",
    "print(\"Our orthonormal basis E:\")\n",
    "print(E)\n",
    "\n",
    "# 1. Check for orthogonality\n",
    "print(\"\\n--- Orthogonality Check (dot products should be ~0) ---\")\n",
    "print(f\"e1 . e2 = {np.dot(e1, e2):.10f}\")\n",
    "print(f\"e1 . e3 = {np.dot(e1, e3):.10f}\")\n",
    "print(f\"e2 . e3 = {np.dot(e2, e3):.10f}\")\n",
    "\n",
    "# A more elegant way is to compute E @ E.T, which should be the identity matrix\n",
    "identity_matrix = E @ E.T\n",
    "print(\"\\nE @ E.T (should be the Identity Matrix):\")\n",
    "print(np.round(identity_matrix, 10)) # Round to handle floating point inaccuracies\n",
    "\n",
    "\n",
    "# 2. Check for normality\n",
    "print(\"\\n--- Normality Check (magnitudes should be 1) ---\")\n",
    "print(f\"||e1|| = {np.linalg.norm(e1):.4f}\")\n",
    "print(f\"||e2|| = {np.linalg.norm(e2):.4f}\")\n",
    "print(f\"||e3|| = {np.linalg.norm(e3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A General Gram-Schmidt Function\n",
    "\n",
    "We can encapsulate the logic into a single Python function that can handle any number of input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthonormal basis from the function:\n",
      "[[ 7.07106781e-01  7.07106781e-01  0.00000000e+00]\n",
      " [-7.07106781e-01  7.07106781e-01  0.00000000e+00]\n",
      " [ 6.66133815e-16 -2.22044605e-16  1.00000000e+00]]\n",
      "\n",
      "Verification (E @ E.T):\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1. -0.]\n",
      " [ 0. -0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def gram_schmidt(vectors):\n",
    "    \"\"\"\n",
    "    Performs the Gram-Schmidt process on a list of vectors.\n",
    "    \n",
    "    Args:\n",
    "        vectors (list of np.array): A list of linearly independent vectors.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: A matrix where each row is an orthonormal basis vector.\n",
    "    \"\"\"\n",
    "    basis = []\n",
    "    for v in vectors:\n",
    "        # Subtract the projections of v onto the existing basis vectors\n",
    "        w = v - sum(np.dot(v, e) * e for e in basis)\n",
    "        \n",
    "        # Check for linear dependence. If w is the zero vector, the original\n",
    "        # vectors were not linearly independent.\n",
    "        if np.linalg.norm(w) > 1e-10: # Use a small tolerance for floating point\n",
    "            basis.append(w / np.linalg.norm(w))\n",
    "    return np.array(basis)\n",
    "\n",
    "# Our original vectors as a list\n",
    "V = [v1, v2, v3]\n",
    "\n",
    "# Apply the function\n",
    "E_func = gram_schmidt(V)\n",
    "\n",
    "print(\"Orthonormal basis from the function:\")\n",
    "print(E_func)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nVerification (E @ E.T):\")\n",
    "print(np.round(E_func @ E_func.T, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Gram-Schmidt process is a powerful and intuitive algorithm for constructing an orthonormal basis from any set of linearly independent vectors. By iteratively subtracting projections and normalizing, it \"straightens out\" the original vectors into a perfectly perpendicular, unit-length framework. This resulting basis simplifies countless problems in linear algebra, numerical analysis, and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
